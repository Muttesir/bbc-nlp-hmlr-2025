{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b97740-a76a-4a8e-bf9a-a0d4d196cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the root directory containing subfolders like 'business', 'sport', etc.\n",
    "data_dir = \"file_path\"\n",
    "\n",
    "# Prepare a list to hold (text, category) tuples\n",
    "data = []\n",
    "\n",
    "# Iterate through each category folder\n",
    "for category in os.listdir(data_dir):\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "\n",
    "    # Ensure it's a directory (e.g., 'business', 'entertainment', etc.)\n",
    "    if os.path.isdir(category_path):\n",
    "        # Iterate through files inside the category folder\n",
    "        for file_name in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "\n",
    "            # Check if it's a regular file\n",
    "            if os.path.isfile(file_path):\n",
    "                # Read the file content\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                    text = file.read().strip()\n",
    "                    # Append text and label to data list\n",
    "                    data.append((text, category))\n",
    "\n",
    "# Create a DataFrame and save it as a CSV file\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
    "df.to_csv(\"bbc_raw_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✅ Dataset saved as 'bbc_raw_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c35014-bca8-4d85-a657-65b1bc055157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import pipeline\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"/content/bbc_raw_dataset.csv\"\n",
    "OUTPUT_DIR = \"/content\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Load the dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def extract_subtopics(df, category, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering on text to extract subtopics.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The full dataset.\n",
    "        category (str): One of the main categories to cluster.\n",
    "        n_clusters (int): Number of topic clusters.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Clustered subtopics with top keywords.\n",
    "    \"\"\"\n",
    "    texts = df[df[\"label\"] == category][\"text\"]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    records = []\n",
    "    for i in range(n_clusters):\n",
    "        top_indices = kmeans.cluster_centers_[i].argsort()[-10:][::-1]\n",
    "        keywords = \", \".join(feature_names[idx] for idx in top_indices)\n",
    "        records.append({\n",
    "            \"category\": category,\n",
    "            \"cluster\": i,\n",
    "            \"keywords\": keywords\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def summarize_april_sentences(df):\n",
    "    \"\"\"\n",
    "    Extract and summarize sentences mentioning 'April'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with full articles.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sentences and summaries containing 'April'.\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    april_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for sentence in re.split(r\"[.?!]\", row[\"text\"]):\n",
    "            if \"April\" in sentence:\n",
    "                clean = sentence.strip().replace(\"\\n\", \" \")\n",
    "                if len(clean) > 30:\n",
    "                    input_len = len(clean.split())\n",
    "                    max_len = max(10, int(input_len * 0.8))\n",
    "                    summary = summarizer(\n",
    "                        clean,\n",
    "                        max_length=max_len,\n",
    "                        min_length=5,\n",
    "                        do_sample=False\n",
    "                    )[0][\"summary_text\"]\n",
    "                    april_data.append({\n",
    "                        \"category\": row[\"label\"],\n",
    "                        \"original\": clean,\n",
    "                        \"summary\": summary\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(april_data)\n",
    "\n",
    "\n",
    "def extract_names(text):\n",
    "    \"\"\"\n",
    "    Extract person-like names using regex.\n",
    "\n",
    "    Args:\n",
    "        text (str): News article text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of names.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\b[A-Z][a-z]+\\s[A-Z][a-z]+\\b\", text)\n",
    "\n",
    "\n",
    "def infer_job(text):\n",
    "    \"\"\"\n",
    "    Infer the job of a named entity based on text context.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full article text.\n",
    "\n",
    "    Returns:\n",
    "        str: Inferred job category.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    if any(w in text for w in [\"singer\", \"album\", \"music\", \"concert\"]):\n",
    "        return \"Musician\"\n",
    "    if any(w in text for w in [\"actor\", \"actress\", \"film\", \"movie\"]):\n",
    "        return \"Actor\"\n",
    "    if any(w in text for w in [\"prime minister\", \"president\", \"parliament\", \"election\"]):\n",
    "        return \"Politician\"\n",
    "    if any(w in text for w in [\"match\", \"player\", \"coach\", \"goal\", \"tournament\"]):\n",
    "        return \"Athlete\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_media_entities(df):\n",
    "    \"\"\"\n",
    "    Extract and classify media personalities from text.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset with news articles.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Name, inferred role, category, and sample context.\n",
    "    \"\"\"\n",
    "    media_records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        names = extract_names(row[\"text\"])\n",
    "        for name in names:\n",
    "            role = infer_job(row[\"text\"])\n",
    "            media_records.append({\n",
    "                \"name\": name,\n",
    "                \"role\": role,\n",
    "                \"category\": row[\"label\"],\n",
    "                \"context\": row[\"text\"][:200]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(media_records).drop_duplicates(subset=[\"name\", \"role\"])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the full pipeline end-to-end.\"\"\"\n",
    "    df = load_dataset(DATA_PATH)\n",
    "\n",
    "    # Step 1: Subtopics\n",
    "    subtopics = pd.concat([\n",
    "        extract_subtopics(df, \"business\"),\n",
    "        extract_subtopics(df, \"entertainment\"),\n",
    "        extract_subtopics(df, \"sport\")\n",
    "    ])\n",
    "    subtopics.to_csv(f\"{OUTPUT_DIR}/subtopics.csv\", index=False)\n",
    "\n",
    "    # Step 2: April summarization\n",
    "    april_df = summarize_april_sentences(df)\n",
    "    april_df.to_csv(f\"{OUTPUT_DIR}/april_summaries.csv\", index=False)\n",
    "\n",
    "    # Step 3: Media personalities\n",
    "    media_df = extract_media_entities(df)\n",
    "    media_df.to_csv(f\"{OUTPUT_DIR}/media_entities.csv\", index=False)\n",
    "\n",
    "    print(\"✅ All tasks completed. Outputs saved in 'outputs/'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4316230-0d4e-48af-9241-ba3f64c3ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
